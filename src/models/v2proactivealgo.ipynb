{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ecd78a",
   "metadata": {},
   "source": [
    "# Proactive autoscaler testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1eff88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 15:23:43.116092: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-08 15:23:43.139447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-08 15:23:43.478327: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 15:23:54.033753: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-01-08 15:23:54.033782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: csemtechgpu\n",
      "2024-01-08 15:23:54.033788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: csemtechgpu\n",
      "2024-01-08 15:23:54.033908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.129.3\n",
      "2024-01-08 15:23:54.033923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.60.13\n",
      "2024-01-08 15:23:54.033927: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 525.60.13 does not match DSO version 535.129.3 -- cannot find working devices in this configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 138ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "0.0046570000000000005 17.316397384000002\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "0\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "0.0066103333333333335 17.316397384000002\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "0\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "0.0066103333333333335 17.316397384000002\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "0\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "0.005085 17.316397384000002\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "0\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f956c555a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "0.007461 17.316397384000002\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "0\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f956c431510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "0.007461 17.316397384000002\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "0\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "0.006698666666666666 17.317699466666667\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "0\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "0.005431666666666667 17.320303632\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "0\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "0.005431666666666667 17.320303632\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "0\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "0.0038286666666666664 17.320303632\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "0\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "166.33249566666666 67.32678844533334\n",
      "curr rps 1\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "78774\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "predicted workload(in rps): 127504\n",
      "500m 512Mi\n",
      "166.33249566666666 67.32678844533334\n",
      "curr rps 78774\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "196080\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "predicted workload(in rps): 277564\n",
      "500m 512Mi\n",
      "441.89897199999996 90.55203697066666\n",
      "curr rps 196080\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "196080\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "predicted workload(in rps): 239311\n",
      "500m 512Mi\n",
      "498.58868966666665 96.95177327733332\n",
      "curr rps 196080\n",
      "max workload per pod:  196080\n",
      "predicted future pods:  1\n",
      "Scaled up to 4 replicas\n",
      "331767\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "predicted workload(in rps): 438239\n",
      "500m 512Mi\n",
      "498.58868966666665 96.95177327733332\n",
      "curr rps 331767\n",
      "max workload per pod:  331767\n",
      "predicted future pods:  1\n",
      "Scaled up to 5 replicas\n",
      "461474\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "predicted workload(in rps): 575020\n",
      "500m 512Mi\n",
      "500.36240666666663 88.50906926666669\n",
      "curr rps 461474\n",
      "max workload per pod:  461474\n",
      "predicted future pods:  1\n",
      "Scaled up to 6 replicas\n",
      "595432\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "predicted workload(in rps): 716665\n",
      "500m 512Mi\n",
      "346.0327666 69.1968395712\n",
      "curr rps 595432\n",
      "max workload per pod:  595432\n",
      "predicted future pods:  1\n",
      "Scaled up to 7 replicas\n",
      "595432\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "predicted workload(in rps): 655992\n",
      "500m 512Mi\n",
      "346.0327666 69.1968395712\n",
      "curr rps 595432\n",
      "max workload per pod:  595432\n",
      "predicted future pods:  1\n",
      "Scaled up to 8 replicas\n",
      "748016\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "predicted workload(in rps): 879600\n",
      "500m 512Mi\n",
      "336.76314716666667 63.26233740133333\n",
      "curr rps 748016\n",
      "max workload per pod:  748016\n",
      "predicted future pods:  1\n",
      "Scaled up to 9 replicas\n",
      "894943\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "predicted workload(in rps): 1022821\n",
      "500m 512Mi\n",
      "252.51013262499998 49.89108773700001\n",
      "curr rps 894943\n",
      "max workload per pod:  894943\n",
      "predicted future pods:  1\n",
      "Scaled up to 10 replicas\n",
      "894943\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "predicted workload(in rps): 956792\n",
      "500m 512Mi\n",
      "226.35835011111112 46.437042196444445\n",
      "curr rps 894943\n",
      "max workload per pod:  894943\n",
      "predicted future pods:  1\n",
      "Scaled up to 11 replicas\n",
      "1035025\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "predicted workload(in rps): 1172230\n",
      "500m 512Mi\n",
      "226.35835011111112 46.437042196444445\n",
      "curr rps 1035025\n",
      "max workload per pod:  1035025\n",
      "predicted future pods:  1\n",
      "Scaled up to 12 replicas\n",
      "1199136\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "predicted workload(in rps): 1344113\n",
      "500m 512Mi\n",
      "326.4821495 61.5132497552\n",
      "curr rps 1199136\n",
      "max workload per pod:  1199136\n",
      "predicted future pods:  1\n",
      "Scaled up to 13 replicas\n",
      "1199136\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "predicted workload(in rps): 1269134\n",
      "500m 512Mi\n",
      "466.38200891666673 73.44397281333335\n",
      "curr rps 1199136\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 12 replicas\n",
      "1473677\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "predicted workload(in rps): 1663863\n",
      "500m 512Mi\n",
      "466.38200891666673 73.44397281333335\n",
      "curr rps 1473677\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 11 replicas\n",
      "1858695\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "predicted workload(in rps): 2109568\n",
      "500m 512Mi\n",
      "489.5495653076922 75.38006957846154\n",
      "curr rps 1858695\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 10 replicas\n",
      "1858695\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "predicted workload(in rps): 1973984\n",
      "500m 512Mi\n",
      "460.9207766923077 75.3322931606154\n",
      "curr rps 1858695\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 9 replicas\n",
      "2107906\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "predicted workload(in rps): 2331349\n",
      "500m 512Mi\n",
      "499.4293778181818 76.96054298254545\n",
      "curr rps 2107906\n",
      "max workload per pod:  2107906\n",
      "predicted future pods:  1\n",
      "Scaled up to 10 replicas\n",
      "2343154\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "predicted workload(in rps): 2567945\n",
      "500m 512Mi\n",
      "500.0190761999999 78.7112878248\n",
      "curr rps 2343154\n",
      "max workload per pod:  2343154\n",
      "predicted future pods:  1\n",
      "Scaled up to 11 replicas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2571118\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "predicted workload(in rps): 2805444\n",
      "500m 512Mi\n",
      "495.38359136363636 81.011677272\n",
      "curr rps 2571118\n",
      "max workload per pod:  2571118\n",
      "predicted future pods:  1\n",
      "Scaled up to 12 replicas\n",
      "2571118\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "predicted workload(in rps): 2705579\n",
      "500m 512Mi\n",
      "495.0016119 82.3331609704\n",
      "curr rps 2571118\n",
      "max workload per pod:  2571118\n",
      "predicted future pods:  1\n",
      "Scaled up to 13 replicas\n",
      "2840982\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "predicted workload(in rps): 3093598\n",
      "500m 512Mi\n",
      "480.38137863636365 80.53440478909091\n",
      "curr rps 2840982\n",
      "max workload per pod:  2840982\n",
      "predicted future pods:  1\n",
      "Scaled up to 14 replicas\n",
      "3107759\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "predicted workload(in rps): 3364883\n",
      "500m 512Mi\n",
      "483.4766772307693 78.9611975526154\n",
      "curr rps 3107759\n",
      "max workload per pod:  3107759\n",
      "predicted future pods:  1\n",
      "Scaled up to 15 replicas\n",
      "3107759\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "predicted workload(in rps): 3252222\n",
      "500m 512Mi\n",
      "496.96305185714294 78.50470310914285\n",
      "curr rps 3107759\n",
      "max workload per pod:  3107759\n",
      "predicted future pods:  1\n",
      "Scaled up to 16 replicas\n",
      "3476234\n",
      "1/1 [==============================] - 0s 259ms/step\n",
      "predicted workload(in rps): 3782990\n",
      "500m 512Mi\n",
      "496.96305185714294 78.50470310914285\n",
      "curr rps 3476234\n",
      "max workload per pod:  3476234\n",
      "predicted future pods:  1\n",
      "Scaled up to 17 replicas\n",
      "4003494\n",
      "1/1 [==============================] - 0s 246ms/step\n",
      "predicted workload(in rps): 4364757\n",
      "500m 512Mi\n",
      "497.76562393333336 79.4390218272\n",
      "curr rps 4003494\n",
      "max workload per pod:  4003494\n",
      "predicted future pods:  1\n",
      "Scaled up to 18 replicas\n",
      "4003494\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "predicted workload(in rps): 4163025\n",
      "500m 512Mi\n",
      "470.2612524705882 77.51949156\n",
      "curr rps 4003494\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 16 replicas\n",
      "4724750\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 5198071\n",
      "500m 512Mi\n",
      "470.2612524705882 77.51949156\n",
      "curr rps 4724750\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 14 replicas\n",
      "5215331\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 5640131\n",
      "500m 512Mi\n",
      "444.77135066666665 72.75213288977778\n",
      "curr rps 5215331\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 12 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 5676063\n",
      "500m 512Mi\n",
      "302.75373800000006 70.89492897955554\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 11 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 5599531\n",
      "500m 512Mi\n",
      "317.48110287500003 74.99752019499999\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 10 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 5585431\n",
      "500m 512Mi\n",
      "17.871579357142856 71.82194983428572\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 9 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "predicted workload(in rps): 5571289\n",
      "500m 512Mi\n",
      "0.6727270833333335 72.663048734\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 8 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "predicted workload(in rps): 5543628\n",
      "500m 512Mi\n",
      "0.4221087272727273 77.51025861018182\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 7 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 5530811\n",
      "500m 512Mi\n",
      "2.068643 55.28278419520001\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 6 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 5493116\n",
      "500m 512Mi\n",
      "2.746621444444444 31.57116439111111\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 5 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 5464392\n",
      "500m 512Mi\n",
      "1.188788375 33.043440113\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 4 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "predicted workload(in rps): 5415509\n",
      "500m 512Mi\n",
      "0.23119228571428568 34.91181248228572\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 5380762\n",
      "500m 512Mi\n",
      "0.2615611666666667 27.865220108\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 5368972\n",
      "500m 512Mi\n",
      "0.004718399999999999 29.571859859200003\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "predicted workload(in rps): 5368972\n",
      "500m 512Mi\n",
      "0.00417625 32.120100742\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "5368971\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 5368972\n",
      "500m 512Mi\n",
      "0.003857666666666667 35.95440867466667\n",
      "curr rps 5368971\n",
      "max workload per pod:  99999999999\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n"
     ]
    }
   ],
   "source": [
    "from asyncio import sleep\n",
    "from prometheus_client import Summary, Counter, start_http_server, Gauge\n",
    "from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY, CollectorRegistry\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "from kubernetes import client, config\n",
    "    # Constants\n",
    "from kubernetes.client.rest import ApiException\n",
    "from pprint import pprint\n",
    "\n",
    "#INTERVAL=input(\"enter the time interval in seconds for next prediction \")\n",
    "output_data = [0,0,0,0,0,0,0,0,0,0]\n",
    "output_from_prom = [0,0,0,0,0,0,0,0,0,0]\n",
    "results=''\n",
    "while True:\n",
    "\n",
    "    # Step 1: Query the data from Prometheus\n",
    "    query = 'haproxy_frontend_http_responses_total'\n",
    "\n",
    "    \n",
    "    #end_time = datetime.utcnow()\n",
    "    #start_time = end_time - timedelta(minutes=10) # last 10 data points\n",
    "    #step = '15s' # adjust this based on your data resolution\n",
    "    #url = 'http://localhost:9091/query_range?query={}&start={}&end={}&step={}'.format(query, start_time.isoformat(), end_time.isoformat(), step)\n",
    "    #response = requests.get(url)\n",
    "    url='http://localhost:9091/api/v1/query?query=haproxy_frontend_http_responses_total'\n",
    "    response = requests.get(url)\n",
    "    #print(response)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()['data']['result']\n",
    "        #print(data[0])\n",
    "    else:\n",
    "        raise PrometheusException(\"Failed to query Prometheus: {}\".format(response.content))\n",
    "    \n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    cur_req_val = int(data[2]['value'][1])\n",
    "    print(cur_req_val)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    results+=','+str(current_time)\n",
    "    results+=','+str(cur_req_val)\n",
    "    \n",
    "\n",
    "    if len(output_data) == 0:\n",
    "        output_data.append(cur_req_val)\n",
    "    else:\n",
    "         \n",
    "        output_data.append(cur_req_val-output_from_prom[-1])\n",
    "        output_data.pop(0)\n",
    "        \n",
    "        output_from_prom.append(cur_req_val)\n",
    "        output_from_prom.pop(0)\n",
    "\n",
    "    #print(output_data,output_from_prom)\n",
    "    \n",
    "    #Step 2: Preprocess the data\n",
    "  \n",
    "\n",
    "    df = pd.DataFrame(output_data)\n",
    "    input_data = df.values # convert to NumPy array\n",
    "    #print(input_data)\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_input_data = scaler.fit_transform(input_data)\n",
    "\n",
    "\n",
    "    input_data_reshaped = np.array(df_input_data).reshape((1,10,1))\n",
    "\n",
    "\n",
    "    #print(input_data_reshaped)\n",
    "\n",
    "\n",
    "\n",
    "    # Step 3: Load the pre-trained modelbi_lstmwith_paper_specs.h5\n",
    "    model = load_model('bi_lstmwith_paper_specs.h5')\n",
    "\n",
    "    # Step 4: Make predictions using the loaded model\n",
    "    #print(\"input to model\",input_data_reshaped)\n",
    "    predictions = model.predict(input_data_reshaped)\n",
    "\n",
    "\n",
    "\n",
    "    testPredict = scaler.inverse_transform(predictions)\n",
    "    predicted_workload=math.ceil(testPredict)\n",
    "    predicted_workload+= cur_req_val\n",
    "    print(\"predicted workload(in rps):\",predicted_workload)\n",
    "    \n",
    "    # step 5:actual pods scaling\n",
    "    NAMESPACE = \"default\"\n",
    "    DEPLOYMENT_NAME = \"loadtestv2\"\n",
    "    CPU_THRESHOLD = 80\n",
    "    RRS = 0.1  # Remove 10% of the surplus pods in case of scaling in\n",
    "    POD_NAME = \"loadtestv2-969c8b8bb-6m2f9\"\n",
    "    CONTAINER_NAME=\"loadtestv2\"\n",
    "    INTERVAL = 60.0  # seconds\n",
    "    pods_minimum=3\n",
    "   \n",
    "\n",
    "\n",
    "    # Initialize Kubernetes client\n",
    "    config.load_kube_config()\n",
    "    app_api = client.AppsV1Api()\n",
    "    api=client.CoreV1Api()\n",
    "    custom_obj_api = client.CustomObjectsApi()\n",
    "\n",
    "    #hpa_api=client.AutoscalingV2Api()\n",
    "\n",
    "\n",
    "\n",
    "    # Helper function to get CPU and memory resources of a pod\n",
    "    def get_pod_resources():\n",
    "        api_response = api.read_namespaced_pod(name=POD_NAME, namespace=NAMESPACE)\n",
    "        cpu_capacity =api_response.spec.containers[0].resources.limits[\"cpu\"]\n",
    "        memory_capacity = api_response.spec.containers[0].resources.limits[\"memory\"]\n",
    "        return cpu_capacity, memory_capacity\n",
    "\n",
    "    # Function to calculate maximum workload\n",
    "    def calculate_max_workload():\n",
    "        cpu_capacity, memory_capacity = get_pod_resources()\n",
    "        print( cpu_capacity, memory_capacity)\n",
    "        cpu_cores = int(cpu_capacity[:-1])# remove the 'm' suffix\n",
    "        memory_mb = int(memory_capacity[:-2])# remove the 'Mi' suffix\n",
    "        cpu_utilization = 1  # assume maximum CPU utilization\n",
    "        memory_usage_per_unit = 1  # assume constant memory usage per unit of workload\n",
    "        max_workload_cpu = cpu_cores * cpu_utilization\n",
    "        max_workload_memory = memory_mb / memory_usage_per_unit\n",
    "        #max_workload = min(max_workload_cpu, max_workload_memory)\n",
    "        max_workload =max_workload_cpu\n",
    "\n",
    "        metrics = api.read_namespaced_pod_status(name=POD_NAME, namespace=NAMESPACE)\n",
    "        resource = custom_obj_api.list_namespaced_custom_object(group=\"metrics.k8s.io\",version=\"v1beta1\", namespace=NAMESPACE, plural=\"pods\")\n",
    "        #pprint(resource)\n",
    "        avg_cpu_usage = 0\n",
    "        avg_mem_usage = 0\n",
    "        count=0\n",
    "        for pod in resource['items']:\n",
    "            for container in pod['containers']:\n",
    "                if container['name']==CONTAINER_NAME:\n",
    "                    count+=1\n",
    "                    #print(container['usage'])\n",
    "                    cpu_usage = container['usage'][\"cpu\"]\n",
    "                    mem_usage = container['usage'][\"memory\"]\n",
    "                    if cpu_usage!='0':\n",
    "                        cpu_usage = float(cpu_usage[:-1])# remove the 'n' suffix\n",
    "                    else:\n",
    "                        cpu_usage= float(cpu_usage)\n",
    "                    if mem_usage!='0':\n",
    "                        mem_usage = float(mem_usage[:-2])# remove the 'Ki' suffix\n",
    "                    else:\n",
    "                        mem_usage=float(mem_usage)\n",
    "                    #convert Ki to Mi and n to m\n",
    "                    mem_usage*=0.000976562\n",
    "                    cpu_usage*=0.000001\n",
    "                    avg_cpu_usage+=cpu_usage\n",
    "                    avg_mem_usage+=mem_usage\n",
    "                    \n",
    "                     \n",
    "                \n",
    "        avg_cpu_usage/=count\n",
    "        avg_mem_usage/=count\n",
    "        print(avg_cpu_usage,avg_mem_usage)\n",
    "        #curr_bottlenecked_resource_utilization=min(cpu_usage,mem_usage)#need this value in 'm' if cpu is considered or in Mi if memory is a bottleneck\n",
    "        curr_bottlenecked_resource_utilization=cpu_usage\n",
    "        if  curr_bottlenecked_resource_utilization==0:\n",
    "            curr_bottlenecked_resource_utilization=1\n",
    "        curr_rps= cur_req_val \n",
    "        if curr_rps==0:\n",
    "            curr_rps=1\n",
    "                    #can fetch the current rps from prometheus\n",
    "        print(\"curr rps\",curr_rps)\n",
    "#         max_workload_per_pod=(max_workload*curr_rps)/curr_bottlenecked_resource_utilization # in rps \n",
    "        max_workload_per_pod=99999999999\n",
    "        if(abs(max_workload-curr_bottlenecked_resource_utilization)<=2):\n",
    "            max_workload_per_pod=curr_rps\n",
    "            \n",
    "        return max_workload_per_pod\n",
    "\n",
    "\n",
    "    # Helper function to get the current number of replicas\n",
    "    def get_replica_count():\n",
    "        \n",
    "        try:\n",
    "            deployment = app_api.read_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
    "            #pprint(deployment)\n",
    "        except ApiException as e:\n",
    "            print(\"Exception when calling AppsV1Api->read_namespaced_deployment: %s\\n\" % e)\n",
    "        \n",
    "        #deployment = app_api.read_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
    "        return deployment.spec.replicas\n",
    "\n",
    "    # Helper function to scale up\n",
    "    def scale_up(replicas):\n",
    "        app_api.patch_namespaced_deployment_scale(\n",
    "            name=DEPLOYMENT_NAME,\n",
    "            namespace=NAMESPACE,\n",
    "            body={\"spec\": {\"replicas\": int(replicas)}}\n",
    "        )\n",
    "        print(f\"Scaled up to {int(replicas)} replicas\")\n",
    "\n",
    "    # Helper function to scale down\n",
    "    def scale_down(replicas):\n",
    "        app_api.patch_namespaced_deployment_scale(\n",
    "            name=DEPLOYMENT_NAME,\n",
    "            namespace=NAMESPACE,\n",
    "            body={\"spec\": {\"replicas\": int(replicas)}}\n",
    "        )\n",
    "        print(f\"Scaled down to {int(replicas)} replicas\")\n",
    "\n",
    "    # Main loop\n",
    "    if True:\n",
    "        max_workload_per_pod = calculate_max_workload() \n",
    "        print(\"max workload per pod: \", max_workload_per_pod)\n",
    "        predicted_future_pods=predicted_workload/max_workload_per_pod\n",
    "        predicted_future_pods=int(predicted_future_pods)\n",
    "        #predicted future pods should be in whole integer number not float\n",
    "        print(\"predicted future pods: \",predicted_future_pods)\n",
    "        current_pods=get_replica_count()\n",
    "\n",
    " \n",
    "\n",
    "        if current_pods+predicted_future_pods>current_pods:\n",
    "            results+=','+str(int(predicted_future_pods+current_pods))\n",
    "            scale_up(predicted_future_pods+current_pods)\n",
    "        elif predicted_future_pods<current_pods:\n",
    "            #print(\"future pods\",future_pods)\n",
    "            future_pods=max(predicted_future_pods,pods_minimum)\n",
    "            pods_surplus=(current_pods-future_pods)*RRS\n",
    "            future_pods_for_scaling_down=current_pods-pods_surplus\n",
    "            results+=','+str(int(future_pods_for_scaling_down))\n",
    "            scale_down(future_pods_for_scaling_down)\n",
    "        else:\n",
    "            print(\"No scaling required\")\n",
    "        #time.sleep(INTERVAL)\n",
    "#     savetxt('results.csv', data, delimiter=',')\n",
    "    results+='\\n'\n",
    "    with open('results.csv','a') as fd:\n",
    "        fd.write(results)\n",
    "    results=''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c37de3",
   "metadata": {},
   "source": [
    "# HPA  Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adb324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 15:38:56.267264: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-08 15:38:56.291068: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-08 15:38:56.636902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 5\n",
      "Running Pods: 5\n",
      "0\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 5\n",
      "Running Pods: 5\n",
      "102526\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 5\n",
      "Running Pods: 5\n",
      "239597\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 7\n",
      "Running Pods: 7\n",
      "526075\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 7\n",
      "Running Pods: 7\n",
      "681773\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 10\n",
      "Running Pods: 7\n",
      "1024130\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 10\n",
      "Running Pods: 10\n",
      "1200679\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 10\n",
      "Running Pods: 10\n",
      "1394645\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 14\n",
      "Running Pods: 14\n",
      "1815428\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 14\n",
      "Running Pods: 14\n",
      "2001189\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 14\n",
      "Running Pods: 14\n",
      "2256506\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 17\n",
      "Running Pods: 17\n",
      "2842718\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 20\n",
      "Running Pods: 20\n",
      "3174001\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 20\n",
      "Running Pods: 20\n",
      "3879112\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 27\n",
      "Running Pods: 27\n",
      "5558239\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 27\n",
      "Running Pods: 27\n",
      "6409920\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 27\n",
      "Running Pods: 27\n",
      "6567483\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 27\n",
      "Running Pods: 27\n",
      "6567483\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 27\n",
      "Running Pods: 27\n",
      "6567483\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 27\n",
      "Running Pods: 27\n",
      "6567483\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 27\n",
      "Running Pods: 27\n",
      "6567483\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 27\n",
      "Running Pods: 27\n"
     ]
    }
   ],
   "source": [
    "from kubernetes import client, config\n",
    "from asyncio import sleep\n",
    "from prometheus_client import Summary, Counter, start_http_server, Gauge\n",
    "from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY, CollectorRegistry\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_running_pods(namespace, deployment_name):\n",
    "    # Load kubeconfig file (or use in-cluster config if running inside a pod)\n",
    "    config.load_kube_config()\n",
    "\n",
    "    # Create Kubernetes API client\n",
    "    api_instance = client.AppsV1Api()\n",
    "    api=client.CoreV1Api()\n",
    "    \n",
    "\n",
    "\n",
    "    try:\n",
    "        # Get the deployment object\n",
    "        deployment = api_instance.read_namespaced_deployment(deployment_name, namespace)\n",
    "\n",
    "        # Get the number of replicas (desired replicas)\n",
    "        desired_replicas = deployment.spec.replicas\n",
    "\n",
    "        # Get the number of currently running pods\n",
    "        label_selector = f\"app={deployment_name}\"\n",
    "        running_pods = api.list_namespaced_pod(\n",
    "            namespace,\n",
    "            label_selector=label_selector,\n",
    "        )\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Deployment: {deployment_name}\")\n",
    "        print(f\"Desired Replicas: {desired_replicas}\")\n",
    "        print(f\"Running Pods: {len(running_pods.items)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "              \n",
    "    return desired_replicas\n",
    "\n",
    "results=''              \n",
    "while(True):\n",
    "    # Step 1: Query the data from Prometheus\n",
    "    query = 'haproxy_frontend_http_responses_total'\n",
    "    url='http://localhost:9091/api/v1/query?query=haproxy_frontend_http_responses_total'\n",
    "    response = requests.get(url)\n",
    "    #print(response)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()['data']['result']\n",
    "        #print(data[0])\n",
    "    else:\n",
    "        raise PrometheusException(\"Failed to query Prometheus: {}\".format(response.content))\n",
    "    \n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    cur_req_val = int(data[2]['value'][1])\n",
    "    print(cur_req_val)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    results+=','+str(current_time)\n",
    "    results+=','+str(cur_req_val)\n",
    "    results+=','+str(get_running_pods(\"default\", \"loadtestv2\"))\n",
    "    results+='\\n'\n",
    "    with open('hpa_results.csv','a') as fd:\n",
    "        fd.write(results)\n",
    "    results=''\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd111",
   "metadata": {},
   "source": [
    "# Dev environment(Proactive autoscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635a3de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500m 512Mi\n",
      "{'cpu': '8441n', 'memory': '15724Ki'}\n",
      "0.008440999999999999 15.355460888000001\n",
      "max workload per pod:  31098211112.42744\n",
      "predicted future pods:  0.00016895946139809523\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 145\u001b[0m\n\u001b[1;32m    143\u001b[0m     pods_surplus\u001b[38;5;241m=\u001b[39m(current_pods\u001b[38;5;241m-\u001b[39mfuture_pods)\u001b[38;5;241m*\u001b[39mRRS\n\u001b[1;32m    144\u001b[0m     future_pods_for_scaling_down\u001b[38;5;241m=\u001b[39mcurrent_pods\u001b[38;5;241m-\u001b[39mpods_surplus\n\u001b[0;32m--> 145\u001b[0m     results\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(future_pods_for_scaling_down))\n\u001b[1;32m    146\u001b[0m     scale_down(future_pods_for_scaling_down)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "from asyncio import sleep\n",
    "from prometheus_client import Summary, Counter, start_http_server, Gauge\n",
    "from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY, CollectorRegistry\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pprint\n",
    "\n",
    "from kubernetes import client, config\n",
    "    # Constants\n",
    "from kubernetes.client.rest import ApiException\n",
    "from pprint import pprint\n",
    "predicted_workload=5254337\n",
    "if(True):\n",
    "    # step 5:actual pods scaling\n",
    "    NAMESPACE = \"default\"\n",
    "    DEPLOYMENT_NAME = \"loadtestv2\"\n",
    "    CPU_THRESHOLD = 80\n",
    "    RRS = 0.1  # Remove 10% of the surplus pods in case of scaling in\n",
    "    POD_NAME = \"loadtestv2-969c8b8bb-bqk69\"\n",
    "    INTERVAL = 60.0  # seconds\n",
    "    pods_minimum=3\n",
    "   \n",
    "\n",
    "\n",
    "    # Initialize Kubernetes client\n",
    "    config.load_kube_config()\n",
    "    app_api = client.AppsV1Api()\n",
    "    api=client.CoreV1Api()\n",
    "    custom_obj_api = client.CustomObjectsApi()\n",
    "\n",
    "    \n",
    "\n",
    "    #hpa_api=client.AutoscalingV2Api()\n",
    "\n",
    "\n",
    "\n",
    "    # Helper function to get CPU and memory resources of a pod\n",
    "    def get_pod_resources():\n",
    "        api_response = api.read_namespaced_pod(name=POD_NAME, namespace=NAMESPACE)\n",
    "        cpu_capacity =api_response.spec.containers[0].resources.limits[\"cpu\"]\n",
    "        memory_capacity = api_response.spec.containers[0].resources.limits[\"memory\"]\n",
    "        return cpu_capacity, memory_capacity\n",
    "\n",
    "    # Function to calculate maximum workload\n",
    "    def calculate_max_workload():\n",
    "        cpu_capacity, memory_capacity = get_pod_resources()\n",
    "        print( cpu_capacity, memory_capacity)\n",
    "        cpu_cores = int(cpu_capacity[:-1])# remove the 'm' suffix\n",
    "        memory_mb = int(memory_capacity[:-2])# remove the 'Mi' suffix\n",
    "        cpu_utilization = 1  # assume maximum CPU utilization\n",
    "        memory_usage_per_unit = 1  # assume constant memory usage per unit of workload\n",
    "        max_workload_cpu = cpu_cores * cpu_utilization\n",
    "        max_workload_memory = memory_mb / memory_usage_per_unit\n",
    "        #max_workload = min(max_workload_cpu, max_workload_memory)\n",
    "        max_workload =max_workload_cpu\n",
    "\n",
    "        metrics = api.read_namespaced_pod_status(name=POD_NAME, namespace=NAMESPACE)\n",
    "        resource = custom_obj_api.list_namespaced_custom_object(group=\"metrics.k8s.io\",version=\"v1beta1\", namespace=NAMESPACE, plural=\"pods\")\n",
    "        #pprint(resource)\n",
    "        for pod in resource['items']:\n",
    "               if pod['metadata']['name']==POD_NAME:\n",
    "                    print(pod['containers'][0]['usage'])\n",
    "                    cpu_usage = pod['containers'][0]['usage'][\"cpu\"]\n",
    "                    mem_usage=pod['containers'][0]['usage'][\"memory\"]\n",
    "                    if cpu_usage!='0':\n",
    "                        cpu_usage = int(cpu_usage[:-1])# remove the 'n' suffix\n",
    "                    else:\n",
    "                        cpu_usage= int(cpu_usage)\n",
    "                    if mem_usage!='0':\n",
    "                        mem_usage = int(mem_usage[:-2])# remove the 'Ki' suffix\n",
    "                    else:\n",
    "                        mem_usage=int(mem_usage)\n",
    "                    #convert Ki to Mi and n to m\n",
    "                    mem_usage*=0.000976562\n",
    "                    cpu_usage*=0.000001    \n",
    "                    print(cpu_usage,mem_usage)\n",
    "      \n",
    "        #curr_bottlenecked_resource_utilization=min(cpu_usage,mem_usage)#need this value in 'm' if cpu is considered or in Mi if memory is a bottleneck\n",
    "        curr_bottlenecked_resource_utilization=cpu_usage\n",
    "        if  curr_bottlenecked_resource_utilization==0:\n",
    "            curr_bottlenecked_resource_utilization=1\n",
    "        curr_rps= cur_req_val \n",
    "        if curr_rps==0:\n",
    "            curr_rps=1\n",
    "                    #can fetch the current rps from prometheus\n",
    "        print(\"curr rps\",curr_rps)\n",
    "#         max_workload_per_pod=(max_workload*curr_rps)/curr_bottlenecked_resource_utilization # in rps \n",
    "        max_workload_per_pod=99999999999\n",
    "        if(abs(max_workload-curr_bottlenecked_resource_utilization)<=2):\n",
    "            max_workload_per_pod=curr_rps\n",
    "            \n",
    "        return max_workload_per_pod\n",
    "\n",
    "\n",
    "\n",
    "    # Helper function to get the current number of replicas\n",
    "    def get_replica_count():\n",
    "        \n",
    "        try:\n",
    "            deployment = app_api.read_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
    "            #pprint(deployment)\n",
    "        except ApiException as e:\n",
    "            print(\"Exception when calling AppsV1Api->read_namespaced_deployment: %s\\n\" % e)\n",
    "        \n",
    "        #deployment = app_api.read_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
    "        return deployment.spec.replicas\n",
    "\n",
    "    # Helper function to scale up\n",
    "    def scale_up(replicas):\n",
    "        app_api.patch_namespaced_deployment_scale(\n",
    "            name=DEPLOYMENT_NAME,\n",
    "            namespace=NAMESPACE,\n",
    "            body={\"spec\": {\"replicas\": int(replicas)}}\n",
    "        )\n",
    "        print(f\"Scaled up to {int(replicas)} replicas\")\n",
    "\n",
    "    # Helper function to scale down\n",
    "    def scale_down(replicas):\n",
    "        app_api.patch_namespaced_deployment_scale(\n",
    "            name=DEPLOYMENT_NAME,\n",
    "            namespace=NAMESPACE,\n",
    "            body={\"spec\": {\"replicas\": int(replicas)}}\n",
    "        )\n",
    "        print(f\"Scaled down to {int(replicas)} replicas\")\n",
    "\n",
    "    # Main loop\n",
    "    if True:\n",
    "        max_workload_per_pod = calculate_max_workload()\n",
    "        print(\"max workload per pod: \", max_workload_per_pod)\n",
    "        predicted_future_pods=predicted_workload/max_workload_per_pod\n",
    "        print(\"predicted future pods: \",predicted_future_pods)\n",
    "        current_pods=get_replica_count()\n",
    " \n",
    "        \n",
    "\n",
    "        if predicted_future_pods>current_pods:\n",
    "            results+=','+str(int(predicted_future_pods))\n",
    "            scale_up(predicted_future_pods)\n",
    "        elif predicted_future_pods<current_pods:\n",
    "            #print(\"future pods\",future_pods)\n",
    "            future_pods=max(predicted_future_pods,pods_minimum)\n",
    "            pods_surplus=(current_pods-future_pods)*RRS\n",
    "            future_pods_for_scaling_down=current_pods-pods_surplus\n",
    "            results+=','+str(int(future_pods_for_scaling_down))\n",
    "            scale_down(future_pods_for_scaling_down)\n",
    "        else:\n",
    "            print(\"No scaling required\")\n",
    "        #time.sleep(INTERVAL)\n",
    "#     savetxt('results.csv', data, delimiter=',')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea567481",
   "metadata": {},
   "source": [
    "# Load testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6d2488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful requests: 100000/100000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "def make_request(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        # You can customize this based on your application's response\n",
    "        if response.status_code == 200:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_concurrent_requests(url, num_requests):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_requests) as executor:\n",
    "        # Use a list to store the futures\n",
    "        futures = [executor.submit(make_request, url) for _ in range(num_requests)]\n",
    "\n",
    "        # Wait for all futures to complete\n",
    "        concurrent.futures.wait(futures)\n",
    "\n",
    "        # Get the results\n",
    "        results = [future.result() for future in futures]\n",
    "\n",
    "    # Count successful requests\n",
    "    successful_requests = sum(results)\n",
    "    print(f\"Successful requests: {successful_requests}/{num_requests}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your endpoint and the number of concurrent requests\n",
    "    endpoint_url = \"http://localhost:9999/hello\"\n",
    "    num_concurrent_requests = 100000\n",
    "\n",
    "    test_concurrent_requests(endpoint_url, num_concurrent_requests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bcb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
