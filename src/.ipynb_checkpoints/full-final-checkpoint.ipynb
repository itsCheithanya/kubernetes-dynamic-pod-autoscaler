{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5baca280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-29 12:15:59.777078: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-04-29 12:15:59.777152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: csemtechgpu\n",
      "2023-04-29 12:15:59.777167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: csemtechgpu\n",
      "2023-04-29 12:15:59.777402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 530.41.3\n",
      "2023-04-29 12:15:59.777440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.60.13\n",
      "2023-04-29 12:15:59.777452: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 525.60.13 does not match DSO version 530.41.3 -- cannot find working devices in this configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 841ms/step\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0] [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]]\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]]\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 18] [0, 0, 0, 0, 0, 0, 1, 1, 1, 19]\n",
      "[[ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [18]]\n",
      "[[[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.05555556]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [1.        ]]]\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "12\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 18, 18] [0, 0, 0, 0, 0, 1, 1, 1, 19, 37]\n",
      "[[ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [18]\n",
      " [18]]\n",
      "[[[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.05555556]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [1.        ]\n",
      "  [1.        ]]]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f50142cdcf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "15\n",
      "[0, 0, 0, 0, 1, 0, 0, 18, 18, 32] [0, 0, 0, 0, 1, 1, 1, 19, 37, 69]\n",
      "[[ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [18]\n",
      " [18]\n",
      " [32]]\n",
      "[[[0.     ]\n",
      "  [0.     ]\n",
      "  [0.     ]\n",
      "  [0.     ]\n",
      "  [0.03125]\n",
      "  [0.     ]\n",
      "  [0.     ]\n",
      "  [0.5625 ]\n",
      "  [0.5625 ]\n",
      "  [1.     ]]]\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f50141e1ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "23\n",
      "[0, 0, 0, 1, 0, 0, 18, 18, 32, 25] [0, 0, 0, 1, 1, 1, 19, 37, 69, 94]\n",
      "[[ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [18]\n",
      " [18]\n",
      " [32]\n",
      " [25]]\n",
      "[[[0.     ]\n",
      "  [0.     ]\n",
      "  [0.     ]\n",
      "  [0.03125]\n",
      "  [0.     ]\n",
      "  [0.     ]\n",
      "  [0.5625 ]\n",
      "  [0.5625 ]\n",
      "  [1.     ]\n",
      "  [0.78125]]]\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PrometheusException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to query Prometheus: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(response\u001b[38;5;241m.\u001b[39mcontent))\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m cur_req_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output_data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from asyncio import sleep\n",
    "from prometheus_client import Summary, Counter, start_http_server, Gauge\n",
    "from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY, CollectorRegistry\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from kubernetes import client, config\n",
    "\n",
    "#INTERVAL=input(\"enter the time interval in seconds for next prediction \")\n",
    "output_data = [0,0,0,0,0,0,0,0,0,0]\n",
    "output_from_prom = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Step 1: Query the data from Prometheus\n",
    "    query = 'node_request_operations_total'\n",
    "\n",
    "    \n",
    "    #end_time = datetime.utcnow()\n",
    "    #start_time = end_time - timedelta(minutes=10) # last 10 data points\n",
    "    #step = '15s' # adjust this based on your data resolution\n",
    "    #url = 'http://localhost:9091/query_range?query={}&start={}&end={}&step={}'.format(query, start_time.isoformat(), end_time.isoformat(), step)\n",
    "    #response = requests.get(url)\n",
    "    url='http://localhost:9091/api/v1/query?query=node_request_operations_total'\n",
    "    response = requests.get(url)\n",
    "    #print(response)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()['data']['result']\n",
    "    else:\n",
    "        raise PrometheusException(\"Failed to query Prometheus: {}\".format(response.content))\n",
    "    \n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    cur_req_val = int(data[0]['value'][1])\n",
    "\n",
    "    if len(output_data) == 0:\n",
    "        output_data.append(cur_req_val)\n",
    "    else:\n",
    "         \n",
    "        output_data.append(cur_req_val-output_from_prom[-1])\n",
    "        output_data.pop(0)\n",
    "        \n",
    "        output_from_prom.append(cur_req_val)\n",
    "        output_from_prom.pop(0)\n",
    "\n",
    "    \n",
    "    #Step 2: Preprocess the data\n",
    "  \n",
    "\n",
    "    df = pd.DataFrame(output_data)\n",
    "    input_data = df.values # convert to NumPy array\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_input_data = scaler.fit_transform(input_data)\n",
    "\n",
    "\n",
    "    input_data_reshaped = np.array(df_input_data).reshape((1,10,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Step 3: Load the pre-trained modelbi_lstmwith_paper_specs.h5\n",
    "    model = load_model('bi_lstmwith_paper_specs.h5')\n",
    "\n",
    "    # Step 4: Make predictions using the loaded model\n",
    "    predictions = model.predict(input_data_reshaped)\n",
    "\n",
    "\n",
    "\n",
    "    testPredict = scaler.inverse_transform(predictions)\n",
    "    predicted_workload=math.ceil(testPredict)\n",
    "    print(predicted_workload)\n",
    "    \n",
    "    \n",
    "    # step 5:actual pods scaling\n",
    "        \n",
    "   \n",
    "    predicted_workload=20\n",
    "\n",
    "    # Constants\n",
    "    NAMESPACE = \"default\"\n",
    "    DEPLOYMENT_NAME = \"nginxapp\"\n",
    "    CPU_THRESHOLD = 80\n",
    "    RRS = 0.1  # Remove 10% of the surplus pods in case of scaling in\n",
    "    POD_NAME = \"nginxapp-7497954958-lgjml\"\n",
    "    INTERVAL = 60.0  # seconds\n",
    "    #hpa_name = \"nginxapp\"\n",
    "\n",
    "\n",
    "    # Initialize Kubernetes client\n",
    "    config.load_kube_config()\n",
    "    app_api = client.AppsV1Api()\n",
    "    api=client.CoreV1Api()\n",
    "\n",
    "    #hpa_api=client.AutoscalingV2Api()\n",
    "\n",
    "\n",
    "\n",
    "    # Helper function to get CPU and memory resources of a pod\n",
    "    def get_pod_resources():\n",
    "        api_response = api.read_namespaced_pod(name=POD_NAME, namespace=NAMESPACE)\n",
    "        cpu_capacity =api_response.spec.containers[0].resources.limits[\"cpu\"]\n",
    "        memory_capacity = api_response.spec.containers[0].resources.limits[\"memory\"]\n",
    "        return cpu_capacity, memory_capacity\n",
    "\n",
    "    # Function to calculate maximum workload\n",
    "    def calculate_max_workload():\n",
    "        cpu_capacity, memory_capacity = get_pod_resources()\n",
    "        cpu_cores = int(cpu_capacity[:-1])# remove the 'm' suffix\n",
    "        memory_mb = int(memory_capacity[:-2])# remove the 'Mi' suffix\n",
    "        cpu_utilization = 1  # assume maximum CPU utilization\n",
    "        memory_usage_per_unit = 1  # assume constant memory usage per unit of workload\n",
    "        max_workload_cpu = cpu_cores * cpu_utilization\n",
    "        max_workload_memory = memory_mb / memory_usage_per_unit\n",
    "        max_workload = min(max_workload_cpu, max_workload_memory)\n",
    "        return max_workload\n",
    "    # def pods_minimum():\n",
    "\n",
    "    #     hpa = hpa_api.read_namespaced_horizontal_pod_autoscaler(name=hpa_name, namespace=\"default\")\n",
    "    #     min_replicas = hpa.spec.min_replicas\n",
    "        #print(f\"The minimum number of replicas for {hpa_name} is {min_replicas}\")\n",
    "    #     api_response = app_api.read_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
    "    #     min_replicas=api_response.spec.replicas\n",
    "\n",
    "    #    return min_replicas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Helper function to get the current number of replicas\n",
    "    def get_replica_count():\n",
    "        deployment = app_api.read_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
    "        return deployment.spec.replicas\n",
    "\n",
    "    # Helper function to scale up\n",
    "    def scale_up(replicas):\n",
    "        app_api.patch_namespaced_deployment_scale(\n",
    "            name=DEPLOYMENT_NAME,\n",
    "            namespace=NAMESPACE,\n",
    "            body={\"spec\": {\"replicas\": replicas}}\n",
    "        )\n",
    "        print(f\"Scaled up to {replicas} replicas\")\n",
    "\n",
    "    # Helper function to scale down\n",
    "    def scale_down(replicas):\n",
    "        api.patch_namespaced_deployment_scale(\n",
    "            name=DEPLOYMENT_NAME,\n",
    "            namespace=NAMESPACE,\n",
    "            body={\"spec\": {\"replicas\": replicas}}\n",
    "        )\n",
    "        print(f\"Scaled down to {replicas} replicas\")\n",
    "\n",
    "    # Main loop\n",
    "    while True:\n",
    "        max_workload_per_pod = calculate_max_workload()\n",
    "\n",
    "        future_pods=predicted_workload/max_workload_per_pod\n",
    "        current_pods=get_replica_count()\n",
    "        if future_pods>current_pods:\n",
    "            scale_up(future_pods)\n",
    "        elif future_pods<current_pods:\n",
    "            print(future_pods)\n",
    "            print(pods_minimum)\n",
    "            #future_pods=max(future_pods,pods_minimum)\n",
    "            pods_surplus=(current_pods-future_pods)*RRS\n",
    "            future_pods=current_pods-pods_surplus\n",
    "            scale_down(future_pods)\n",
    "        else:\n",
    "            print(\"No scaling required\")\n",
    "        #time.sleep(INTERVAL)\n",
    "    \n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c530506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
