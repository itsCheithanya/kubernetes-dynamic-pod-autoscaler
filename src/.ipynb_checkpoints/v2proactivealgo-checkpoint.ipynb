{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ecd78a",
   "metadata": {},
   "source": [
    "# Proactive autoscaler testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1eff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140294\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "predicted workload(in rps): 86785\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '63224Ki'}\n",
      "0.0 61.742155888000006\n",
      "max workload per pod:  1136128.12819893\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "predicted workload(in rps): 48230\n",
      "500m 512Mi\n",
      "{'cpu': '6968n', 'memory': '63224Ki'}\n",
      "0.006967999999999999 61.742155888000006\n",
      "max workload per pod:  10067020665.901264\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 42400\n",
      "500m 512Mi\n",
      "{'cpu': '6026n', 'memory': '63224Ki'}\n",
      "0.006026 61.742155888000006\n",
      "max workload per pod:  11640723531.364088\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7febf84acb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "predicted workload(in rps): 35951\n",
      "500m 512Mi\n",
      "{'cpu': '8579n', 'memory': '63224Ki'}\n",
      "0.008579 61.742155888000006\n",
      "max workload per pod:  8176594008.625714\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7febf84af910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "predicted workload(in rps): 30893\n",
      "500m 512Mi\n",
      "{'cpu': '8579n', 'memory': '63224Ki'}\n",
      "0.008579 61.742155888000006\n",
      "max workload per pod:  8176594008.625714\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 26524\n",
      "500m 512Mi\n",
      "{'cpu': '8579n', 'memory': '63224Ki'}\n",
      "0.008579 61.742155888000006\n",
      "max workload per pod:  8176594008.625714\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 22591\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '63224Ki'}\n",
      "0.0 61.742155888000006\n",
      "max workload per pod:  1136128.12819893\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 18753\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '63224Ki'}\n",
      "0.0 61.742155888000006\n",
      "max workload per pod:  1136128.12819893\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "predicted workload(in rps): 14856\n",
      "500m 512Mi\n",
      "{'cpu': '4982n', 'memory': '63224Ki'}\n",
      "0.0049819999999999994 61.742155888000006\n",
      "max workload per pod:  14080088317.944603\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "predicted workload(in rps): 10767\n",
      "500m 512Mi\n",
      "{'cpu': '9876n', 'memory': '63224Ki'}\n",
      "0.009876 61.742155888000006\n",
      "max workload per pod:  7102774402.592143\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '9876n', 'memory': '63224Ki'}\n",
      "0.009876 61.742155888000006\n",
      "max workload per pod:  7102774402.592143\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '63224Ki'}\n",
      "0.0 61.742155888000006\n",
      "max workload per pod:  1136128.12819893\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '63224Ki'}\n",
      "0.0 61.742155888000006\n",
      "max workload per pod:  1136128.12819893\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '63224Ki'}\n",
      "0.0 61.742155888000006\n",
      "max workload per pod:  1136128.12819893\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '5808n', 'memory': '63224Ki'}\n",
      "0.005808 61.742155888000006\n",
      "max workload per pod:  12077651515.151516\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '63224Ki'}\n",
      "0.0 61.742155888000006\n",
      "max workload per pod:  1136128.12819893\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '63224Ki'}\n",
      "0.0 61.742155888000006\n",
      "max workload per pod:  1136128.12819893\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '63224Ki'}\n",
      "0.0 61.742155888000006\n",
      "max workload per pod:  1136128.12819893\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '43452069n', 'memory': '61024Ki'}\n",
      "43.452068999999995 59.593719488000005\n",
      "max workload per pod:  1614353.5075395377\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '43452069n', 'memory': '61024Ki'}\n",
      "43.452068999999995 59.593719488000005\n",
      "max workload per pod:  1614353.5075395377\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '72489862n', 'memory': '61924Ki'}\n",
      "72.489862 60.472625288\n",
      "max workload per pod:  1159979.4066476515\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '70224103n', 'memory': '61996Ki'}\n",
      "70.224103 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '70224103n', 'memory': '61996Ki'}\n",
      "70.224103 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '71798432n', 'memory': '61988Ki'}\n",
      "71.79843199999999 60.535125256\n",
      "max workload per pod:  1158781.776751132\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '11737954n', 'memory': '61996Ki'}\n",
      "11.737954 60.542937752\n",
      "max workload per pod:  5976084.077344314\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '10992n', 'memory': '61996Ki'}\n",
      "0.010992 60.542937752\n",
      "max workload per pod:  6381641193.595342\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '10992n', 'memory': '61996Ki'}\n",
      "0.010992 60.542937752\n",
      "max workload per pod:  6381641193.595342\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '9053n', 'memory': '61996Ki'}\n",
      "0.009053 60.542937752\n",
      "max workload per pod:  7748481166.464155\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '9053n', 'memory': '61996Ki'}\n",
      "0.009053 60.542937752\n",
      "max workload per pod:  7748481166.464155\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '6597n', 'memory': '61996Ki'}\n",
      "0.0065969999999999996 60.542937752\n",
      "max workload per pod:  10633166590.874641\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '6597n', 'memory': '61996Ki'}\n",
      "0.0065969999999999996 60.542937752\n",
      "max workload per pod:  10633166590.874641\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '6001n', 'memory': '61996Ki'}\n",
      "0.006000999999999999 60.542937752\n",
      "max workload per pod:  11689218463.589403\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '12954n', 'memory': '61996Ki'}\n",
      "0.012954 60.542937752\n",
      "max workload per pod:  5415084143.893778\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '12954n', 'memory': '61996Ki'}\n",
      "0.012954 60.542937752\n",
      "max workload per pod:  5415084143.893778\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '7250n', 'memory': '61996Ki'}\n",
      "0.0072499999999999995 60.542937752\n",
      "max workload per pod:  9675448275.86207\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61996Ki'}\n",
      "0.0 60.542937752\n",
      "max workload per pod:  1158632.2468747848\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '6141n', 'memory': '61996Ki'}\n",
      "0.006141 60.542937752\n",
      "max workload per pod:  11422732453.99772\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '20837986n', 'memory': '48216Ki'}\n",
      "20.837986 47.085913392\n",
      "max workload per pod:  3366304.210013386\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '20837986n', 'memory': '48216Ki'}\n",
      "20.837986 47.085913392\n",
      "max workload per pod:  3366304.210013386\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '41717143n', 'memory': '61636Ki'}\n",
      "41.717143 60.191375432\n",
      "max workload per pod:  1681490.9880094137\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '41717143n', 'memory': '61636Ki'}\n",
      "41.717143 60.191375432\n",
      "max workload per pod:  1681490.9880094137\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '41717143n', 'memory': '61636Ki'}\n",
      "41.717143 60.191375432\n",
      "max workload per pod:  1681490.9880094137\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '52844711n', 'memory': '61784Ki'}\n",
      "52.844711 60.335906608\n",
      "max workload per pod:  1327417.6104397657\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '21935422n', 'memory': '61960Ki'}\n",
      "21.935422 60.50778152\n",
      "max workload per pod:  3197886.961098811\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '21935422n', 'memory': '61960Ki'}\n",
      "21.935422 60.50778152\n",
      "max workload per pod:  3197886.961098811\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '49299n', 'memory': '61956Ki'}\n",
      "0.049298999999999996 60.503875272\n",
      "max workload per pod:  1422888902.4118137\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61956Ki'}\n",
      "0.0 60.503875272\n",
      "max workload per pod:  1159380.2824141192\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61956Ki'}\n",
      "0.0 60.503875272\n",
      "max workload per pod:  1159380.2824141192\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cpu': '5467n', 'memory': '61956Ki'}\n",
      "0.005467 60.503875272\n",
      "max workload per pod:  12830985915.492958\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '5467n', 'memory': '61956Ki'}\n",
      "0.005467 60.503875272\n",
      "max workload per pod:  12830985915.492958\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '5467n', 'memory': '61956Ki'}\n",
      "0.005467 60.503875272\n",
      "max workload per pod:  12830985915.492958\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '9125n', 'memory': '61956Ki'}\n",
      "0.009125 60.503875272\n",
      "max workload per pod:  7687342465.753426\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61956Ki'}\n",
      "0.0 60.503875272\n",
      "max workload per pod:  1159380.2824141192\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61956Ki'}\n",
      "0.0 60.503875272\n",
      "max workload per pod:  1159380.2824141192\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '7809n', 'memory': '61956Ki'}\n",
      "0.007809 60.503875272\n",
      "max workload per pod:  8982840312.459982\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61956Ki'}\n",
      "0.0 60.503875272\n",
      "max workload per pod:  1159380.2824141192\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61956Ki'}\n",
      "0.0 60.503875272\n",
      "max workload per pod:  1159380.2824141192\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61956Ki'}\n",
      "0.0 60.503875272\n",
      "max workload per pod:  1159380.2824141192\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '6986n', 'memory': '61956Ki'}\n",
      "0.006986 60.503875272\n",
      "max workload per pod:  10041082164.328657\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '10054n', 'memory': '61956Ki'}\n",
      "0.010053999999999999 60.503875272\n",
      "max workload per pod:  6977024070.021883\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '10054n', 'memory': '61956Ki'}\n",
      "0.010053999999999999 60.503875272\n",
      "max workload per pod:  6977024070.021883\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61956Ki'}\n",
      "0.0 60.503875272\n",
      "max workload per pod:  1159380.2824141192\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '6632n', 'memory': '61956Ki'}\n",
      "0.006632 60.503875272\n",
      "max workload per pod:  10577050663.44994\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '6632n', 'memory': '61956Ki'}\n",
      "0.006632 60.503875272\n",
      "max workload per pod:  10577050663.44994\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '6632n', 'memory': '61956Ki'}\n",
      "0.006632 60.503875272\n",
      "max workload per pod:  10577050663.44994\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '14989714n', 'memory': '51364Ki'}\n",
      "14.989714 50.160130568\n",
      "max workload per pod:  4679675.676267073\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '14989714n', 'memory': '51364Ki'}\n",
      "14.989714 50.160130568\n",
      "max workload per pod:  4679675.676267073\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '42374023n', 'memory': '60020Ki'}\n",
      "42.374023 58.613251240000004\n",
      "max workload per pod:  1655424.598226135\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '30259685n', 'memory': '62916Ki'}\n",
      "30.259684999999998 61.441374792000005\n",
      "max workload per pod:  2318166.8943348224\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '30259685n', 'memory': '62916Ki'}\n",
      "30.259684999999998 61.441374792000005\n",
      "max workload per pod:  2318166.8943348224\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '25291739n', 'memory': '60900Ki'}\n",
      "25.291739 59.4726258\n",
      "max workload per pod:  2773514.3083676454\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '31485586n', 'memory': '61416Ki'}\n",
      "31.485585999999998 59.976531792\n",
      "max workload per pod:  2227908.3514596173\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '31485586n', 'memory': '61416Ki'}\n",
      "31.485585999999998 59.976531792\n",
      "max workload per pod:  2227908.3514596173\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '39452n', 'memory': '61408Ki'}\n",
      "0.039452 59.968719296\n",
      "max workload per pod:  1778034066.7139814\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '39452n', 'memory': '61408Ki'}\n",
      "0.039452 59.968719296\n",
      "max workload per pod:  1778034066.7139814\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '39452n', 'memory': '61408Ki'}\n",
      "0.039452 59.968719296\n",
      "max workload per pod:  1778034066.7139814\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '39269232n', 'memory': '62400Ki'}\n",
      "39.269231999999995 60.937468800000005\n",
      "max workload per pod:  1786309.444503524\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '28629218n', 'memory': '61404Ki'}\n",
      "28.629217999999998 59.964813048\n",
      "max workload per pod:  2450189.173871253\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '18582810n', 'memory': '62320Ki'}\n",
      "18.58281 60.85934384\n",
      "max workload per pod:  3774832.7621064847\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '18582810n', 'memory': '62320Ki'}\n",
      "18.58281 60.85934384\n",
      "max workload per pod:  3774832.7621064847\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '28362880n', 'memory': '61264Ki'}\n",
      "28.362879999999997 59.828094368\n",
      "max workload per pod:  2473197.3621860687\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61264Ki'}\n",
      "0.0 59.828094368\n",
      "max workload per pod:  1172475.920234545\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61264Ki'}\n",
      "0.0 59.828094368\n",
      "max workload per pod:  1172475.920234545\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '9427n', 'memory': '61264Ki'}\n",
      "0.009427 59.828094368\n",
      "max workload per pod:  7441073512.252043\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '9866n', 'memory': '61264Ki'}\n",
      "0.009866 59.828094368\n",
      "max workload per pod:  7109973646.8680315\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '9866n', 'memory': '61264Ki'}\n",
      "0.009866 59.828094368\n",
      "max workload per pod:  7109973646.8680315\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61264Ki'}\n",
      "0.0 59.828094368\n",
      "max workload per pod:  1172475.920234545\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61264Ki'}\n",
      "0.0 59.828094368\n",
      "max workload per pod:  1172475.920234545\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61264Ki'}\n",
      "0.0 59.828094368\n",
      "max workload per pod:  1172475.920234545\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '7888n', 'memory': '61264Ki'}\n",
      "0.007888 59.828094368\n",
      "max workload per pod:  8892875253.549696\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n",
      "140294\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "predicted workload(in rps): 1\n",
      "500m 512Mi\n",
      "{'cpu': '0', 'memory': '61264Ki'}\n",
      "0.0 59.828094368\n",
      "max workload per pod:  1172475.920234545\n",
      "predicted future pods:  0\n",
      "Scaled down to 3 replicas\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 45\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#print(data[0])\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PrometheusException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to query Prometheus: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(response\u001b[38;5;241m.\u001b[39mcontent))\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m cur_req_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_req_val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from asyncio import sleep\n",
    "from prometheus_client import Summary, Counter, start_http_server, Gauge\n",
    "from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY, CollectorRegistry\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "from kubernetes import client, config\n",
    "    # Constants\n",
    "from kubernetes.client.rest import ApiException\n",
    "from pprint import pprint\n",
    "\n",
    "#INTERVAL=input(\"enter the time interval in seconds for next prediction \")\n",
    "output_data = [0,0,0,0,0,0,0,0,0,0]\n",
    "output_from_prom = [0,0,0,0,0,0,0,0,0,0]\n",
    "results=''\n",
    "while True:\n",
    "\n",
    "    # Step 1: Query the data from Prometheus\n",
    "    query = 'haproxy_frontend_http_responses_total'\n",
    "\n",
    "    \n",
    "    #end_time = datetime.utcnow()\n",
    "    #start_time = end_time - timedelta(minutes=10) # last 10 data points\n",
    "    #step = '15s' # adjust this based on your data resolution\n",
    "    #url = 'http://localhost:9091/query_range?query={}&start={}&end={}&step={}'.format(query, start_time.isoformat(), end_time.isoformat(), step)\n",
    "    #response = requests.get(url)\n",
    "    url='http://localhost:9091/api/v1/query?query=haproxy_frontend_http_responses_total'\n",
    "    response = requests.get(url)\n",
    "    #print(response)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()['data']['result']\n",
    "        #print(data[0])\n",
    "    else:\n",
    "        raise PrometheusException(\"Failed to query Prometheus: {}\".format(response.content))\n",
    "    \n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    cur_req_val = int(data[2]['value'][1])\n",
    "    print(cur_req_val)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    results+=','+str(current_time)\n",
    "    results+=','+str(cur_req_val)\n",
    "    \n",
    "\n",
    "    if len(output_data) == 0:\n",
    "        output_data.append(cur_req_val)\n",
    "    else:\n",
    "         \n",
    "        output_data.append(cur_req_val-output_from_prom[-1])\n",
    "        output_data.pop(0)\n",
    "        \n",
    "        output_from_prom.append(cur_req_val)\n",
    "        output_from_prom.pop(0)\n",
    "\n",
    "    #print(output_data,output_from_prom)\n",
    "    \n",
    "    #Step 2: Preprocess the data\n",
    "  \n",
    "\n",
    "    df = pd.DataFrame(output_data)\n",
    "    input_data = df.values # convert to NumPy array\n",
    "    #print(input_data)\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_input_data = scaler.fit_transform(input_data)\n",
    "\n",
    "\n",
    "    input_data_reshaped = np.array(df_input_data).reshape((1,10,1))\n",
    "\n",
    "\n",
    "    #print(input_data_reshaped)\n",
    "\n",
    "\n",
    "\n",
    "    # Step 3: Load the pre-trained modelbi_lstmwith_paper_specs.h5\n",
    "    model = load_model('bi_lstmwith_paper_specs.h5')\n",
    "\n",
    "    # Step 4: Make predictions using the loaded model\n",
    "    predictions = model.predict(input_data_reshaped)\n",
    "\n",
    "\n",
    "\n",
    "    testPredict = scaler.inverse_transform(predictions)\n",
    "    predicted_workload=math.ceil(testPredict)\n",
    "    print(\"predicted workload(in rps):\",predicted_workload)\n",
    "    \n",
    "    # step 5:actual pods scaling\n",
    "    NAMESPACE = \"default\"\n",
    "    DEPLOYMENT_NAME = \"loadtestv2\"\n",
    "    CPU_THRESHOLD = 80\n",
    "    RRS = 0.1  # Remove 10% of the surplus pods in case of scaling in\n",
    "    POD_NAME = \"loadtestv2-969c8b8bb-bqk69\"\n",
    "    INTERVAL = 60.0  # seconds\n",
    "    pods_minimum=3\n",
    "   \n",
    "\n",
    "\n",
    "    # Initialize Kubernetes client\n",
    "    config.load_kube_config()\n",
    "    app_api = client.AppsV1Api()\n",
    "    api=client.CoreV1Api()\n",
    "    custom_obj_api = client.CustomObjectsApi()\n",
    "\n",
    "    #hpa_api=client.AutoscalingV2Api()\n",
    "\n",
    "\n",
    "\n",
    "    # Helper function to get CPU and memory resources of a pod\n",
    "    def get_pod_resources():\n",
    "        api_response = api.read_namespaced_pod(name=POD_NAME, namespace=NAMESPACE)\n",
    "        cpu_capacity =api_response.spec.containers[0].resources.limits[\"cpu\"]\n",
    "        memory_capacity = api_response.spec.containers[0].resources.limits[\"memory\"]\n",
    "        return cpu_capacity, memory_capacity\n",
    "\n",
    "    # Function to calculate maximum workload\n",
    "    def calculate_max_workload():\n",
    "        cpu_capacity, memory_capacity = get_pod_resources()\n",
    "        print( cpu_capacity, memory_capacity)\n",
    "        cpu_cores = int(cpu_capacity[:-1])# remove the 'm' suffix\n",
    "        memory_mb = int(memory_capacity[:-2])# remove the 'Mi' suffix\n",
    "        cpu_utilization = 1  # assume maximum CPU utilization\n",
    "        memory_usage_per_unit = 1  # assume constant memory usage per unit of workload\n",
    "        max_workload_cpu = cpu_cores * cpu_utilization\n",
    "        max_workload_memory = memory_mb / memory_usage_per_unit\n",
    "        max_workload = min(max_workload_cpu, max_workload_memory)\n",
    "        \n",
    "\n",
    "        metrics = api.read_namespaced_pod_status(name=POD_NAME, namespace=NAMESPACE)\n",
    "        resource = custom_obj_api.list_namespaced_custom_object(group=\"metrics.k8s.io\",version=\"v1beta1\", namespace=NAMESPACE, plural=\"pods\")\n",
    "        #pprint(resource)\n",
    "        for pod in resource['items']:\n",
    "               if pod['metadata']['name']==POD_NAME:\n",
    "                    print(pod['containers'][0]['usage'])\n",
    "                    cpu_usage = pod['containers'][0]['usage'][\"cpu\"]\n",
    "                    mem_usage=pod['containers'][0]['usage'][\"memory\"]\n",
    "                    if cpu_usage!='0':\n",
    "                        cpu_usage = int(cpu_usage[:-1])# remove the 'n' suffix\n",
    "                    else:\n",
    "                        cpu_usage= int(cpu_usage)\n",
    "                    if mem_usage!='0':\n",
    "                        mem_usage = int(mem_usage[:-2])# remove the 'Ki' suffix\n",
    "                    else:\n",
    "                        mem_usage=int(mem_usage)\n",
    "                    #convert Ki to Mi and n to m\n",
    "                    mem_usage*=0.000976562\n",
    "                    cpu_usage*=0.000001    \n",
    "                    print(cpu_usage,mem_usage)\n",
    "      \n",
    "        curr_bottlenecked_resource_utilization=min(cpu_usage,mem_usage)#need this value in 'm' if cpu is considered or in Mi if memory is a bottleneck\n",
    "        if  curr_bottlenecked_resource_utilization==0:\n",
    "            curr_bottlenecked_resource_utilization=max(cpu_usage,mem_usage)\n",
    "        curr_rps= cur_req_val             #can fetch the current rps from prometheus\n",
    "        max_workload_per_pod=(max_workload*curr_rps)/curr_bottlenecked_resource_utilization # in rps \n",
    "        return max_workload_per_pod\n",
    "\n",
    "\n",
    "    # Helper function to get the current number of replicas\n",
    "    def get_replica_count():\n",
    "        \n",
    "        try:\n",
    "            deployment = app_api.read_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
    "            #pprint(deployment)\n",
    "        except ApiException as e:\n",
    "            print(\"Exception when calling AppsV1Api->read_namespaced_deployment: %s\\n\" % e)\n",
    "        \n",
    "        #deployment = app_api.read_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
    "        return deployment.spec.replicas\n",
    "\n",
    "    # Helper function to scale up\n",
    "    def scale_up(replicas):\n",
    "        app_api.patch_namespaced_deployment_scale(\n",
    "            name=DEPLOYMENT_NAME,\n",
    "            namespace=NAMESPACE,\n",
    "            body={\"spec\": {\"replicas\": int(replicas)}}\n",
    "        )\n",
    "        print(f\"Scaled up to {int(replicas)} replicas\")\n",
    "\n",
    "    # Helper function to scale down\n",
    "    def scale_down(replicas):\n",
    "        app_api.patch_namespaced_deployment_scale(\n",
    "            name=DEPLOYMENT_NAME,\n",
    "            namespace=NAMESPACE,\n",
    "            body={\"spec\": {\"replicas\": int(replicas)}}\n",
    "        )\n",
    "        print(f\"Scaled down to {int(replicas)} replicas\")\n",
    "\n",
    "    # Main loop\n",
    "    if True:\n",
    "        max_workload_per_pod = calculate_max_workload()\n",
    "        print(\"max workload per pod: \", max_workload_per_pod)\n",
    "        predicted_future_pods=predicted_workload/max_workload_per_pod\n",
    "        predicted_future_pods=int(predicted_future_pods)\n",
    "        #predicted future pods should be in whole integer number not float\n",
    "        print(\"predicted future pods: \",predicted_future_pods)\n",
    "        current_pods=get_replica_count()\n",
    "\n",
    " \n",
    "\n",
    "        if predicted_future_pods>current_pods:\n",
    "            results+=','+str(int(predicted_future_pods))\n",
    "            scale_up(predicted_future_pods)\n",
    "        elif predicted_future_pods<current_pods:\n",
    "            #print(\"future pods\",future_pods)\n",
    "            future_pods=max(predicted_future_pods,pods_minimum)\n",
    "            pods_surplus=(current_pods-future_pods)*RRS\n",
    "            future_pods_for_scaling_down=current_pods-pods_surplus\n",
    "            results+=','+str(int(future_pods_for_scaling_down))\n",
    "            scale_down(future_pods_for_scaling_down)\n",
    "        else:\n",
    "            print(\"No scaling required\")\n",
    "        #time.sleep(INTERVAL)\n",
    "#     savetxt('results.csv', data, delimiter=',')\n",
    "    results+='\\n'\n",
    "    with open('results.csv','a') as fd:\n",
    "        fd.write(results)\n",
    "    results=''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c37de3",
   "metadata": {},
   "source": [
    "# HPA  Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8adb324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n",
      "140294\n",
      "Deployment: loadtestv2\n",
      "Desired Replicas: 3\n",
      "Running Pods: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 62\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m#print(data[0])\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PrometheusException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to query Prometheus: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(response\u001b[38;5;241m.\u001b[39mcontent))\n\u001b[0;32m---> 62\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m cur_req_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(cur_req_val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kubernetes import client, config\n",
    "from asyncio import sleep\n",
    "from prometheus_client import Summary, Counter, start_http_server, Gauge\n",
    "from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY, CollectorRegistry\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_running_pods(namespace, deployment_name):\n",
    "    # Load kubeconfig file (or use in-cluster config if running inside a pod)\n",
    "    config.load_kube_config()\n",
    "\n",
    "    # Create Kubernetes API client\n",
    "    api_instance = client.AppsV1Api()\n",
    "    api=client.CoreV1Api()\n",
    "    \n",
    "\n",
    "\n",
    "    try:\n",
    "        # Get the deployment object\n",
    "        deployment = api_instance.read_namespaced_deployment(deployment_name, namespace)\n",
    "\n",
    "        # Get the number of replicas (desired replicas)\n",
    "        desired_replicas = deployment.spec.replicas\n",
    "\n",
    "        # Get the number of currently running pods\n",
    "        label_selector = f\"app={deployment_name}\"\n",
    "        running_pods = api.list_namespaced_pod(\n",
    "            namespace,\n",
    "            label_selector=label_selector,\n",
    "        )\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Deployment: {deployment_name}\")\n",
    "        print(f\"Desired Replicas: {desired_replicas}\")\n",
    "        print(f\"Running Pods: {len(running_pods.items)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "              \n",
    "    return desired_replicas\n",
    "\n",
    "results=''              \n",
    "while(True):\n",
    "    # Step 1: Query the data from Prometheus\n",
    "    query = 'haproxy_frontend_http_responses_total'\n",
    "    url='http://localhost:9091/api/v1/query?query=haproxy_frontend_http_responses_total'\n",
    "    response = requests.get(url)\n",
    "    #print(response)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()['data']['result']\n",
    "        #print(data[0])\n",
    "    else:\n",
    "        raise PrometheusException(\"Failed to query Prometheus: {}\".format(response.content))\n",
    "    \n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    cur_req_val = int(data[2]['value'][1])\n",
    "    print(cur_req_val)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    results+=','+str(current_time)\n",
    "    results+=','+str(cur_req_val)\n",
    "    results+=','+str(get_running_pods(\"default\", \"loadtestv2\"))\n",
    "    results+='\\n'\n",
    "    with open('hpa_results.csv','a') as fd:\n",
    "        fd.write(results)\n",
    "    results=''\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd111",
   "metadata": {},
   "source": [
    "# Dev environment(Proactive autoscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635a3de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500m 512Mi\n",
      "{'cpu': '8441n', 'memory': '15724Ki'}\n",
      "0.008440999999999999 15.355460888000001\n",
      "max workload per pod:  31098211112.42744\n",
      "predicted future pods:  0.00016895946139809523\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 145\u001b[0m\n\u001b[1;32m    143\u001b[0m     pods_surplus\u001b[38;5;241m=\u001b[39m(current_pods\u001b[38;5;241m-\u001b[39mfuture_pods)\u001b[38;5;241m*\u001b[39mRRS\n\u001b[1;32m    144\u001b[0m     future_pods_for_scaling_down\u001b[38;5;241m=\u001b[39mcurrent_pods\u001b[38;5;241m-\u001b[39mpods_surplus\n\u001b[0;32m--> 145\u001b[0m     results\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(future_pods_for_scaling_down))\n\u001b[1;32m    146\u001b[0m     scale_down(future_pods_for_scaling_down)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "from asyncio import sleep\n",
    "from prometheus_client import Summary, Counter, start_http_server, Gauge\n",
    "from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily, REGISTRY, CollectorRegistry\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import pprint\n",
    "\n",
    "from kubernetes import client, config\n",
    "    # Constants\n",
    "from kubernetes.client.rest import ApiException\n",
    "from pprint import pprint\n",
    "predicted_workload=5254337\n",
    "if(True):\n",
    "    # step 5:actual pods scaling\n",
    "    NAMESPACE = \"default\"\n",
    "    DEPLOYMENT_NAME = \"loadtestv2\"\n",
    "    CPU_THRESHOLD = 80\n",
    "    RRS = 0.1  # Remove 10% of the surplus pods in case of scaling in\n",
    "    POD_NAME = \"loadtestv2-969c8b8bb-bqk69\"\n",
    "    INTERVAL = 60.0  # seconds\n",
    "    pods_minimum=3\n",
    "   \n",
    "\n",
    "\n",
    "    # Initialize Kubernetes client\n",
    "    config.load_kube_config()\n",
    "    app_api = client.AppsV1Api()\n",
    "    api=client.CoreV1Api()\n",
    "    custom_obj_api = client.CustomObjectsApi()\n",
    "\n",
    "    \n",
    "\n",
    "    #hpa_api=client.AutoscalingV2Api()\n",
    "\n",
    "\n",
    "\n",
    "    # Helper function to get CPU and memory resources of a pod\n",
    "    def get_pod_resources():\n",
    "        api_response = api.read_namespaced_pod(name=POD_NAME, namespace=NAMESPACE)\n",
    "        cpu_capacity =api_response.spec.containers[0].resources.limits[\"cpu\"]\n",
    "        memory_capacity = api_response.spec.containers[0].resources.limits[\"memory\"]\n",
    "        return cpu_capacity, memory_capacity\n",
    "\n",
    "    # Function to calculate maximum workload\n",
    "    def calculate_max_workload():\n",
    "        cpu_capacity, memory_capacity = get_pod_resources()\n",
    "        print( cpu_capacity, memory_capacity)\n",
    "        cpu_cores = int(cpu_capacity[:-1])# remove the 'm' suffix\n",
    "        memory_mb = int(memory_capacity[:-2])# remove the 'Mi' suffix\n",
    "        cpu_utilization = 1  # assume maximum CPU utilization\n",
    "        memory_usage_per_unit = 1  # assume constant memory usage per unit of workload\n",
    "        max_workload_cpu = cpu_cores * cpu_utilization\n",
    "        max_workload_memory = memory_mb / memory_usage_per_unit\n",
    "        max_workload = min(max_workload_cpu, max_workload_memory)\n",
    "        \n",
    "\n",
    "        metrics = api.read_namespaced_pod_status(name=POD_NAME, namespace=NAMESPACE)\n",
    "        resource = custom_obj_api.list_namespaced_custom_object(group=\"metrics.k8s.io\",version=\"v1beta1\", namespace=NAMESPACE, plural=\"pods\")\n",
    "        #pprint(resource)\n",
    "        for pod in resource['items']:\n",
    "               if pod['metadata']['name']==POD_NAME:\n",
    "                    print(pod['containers'][0]['usage'])\n",
    "                    cpu_usage = pod['containers'][0]['usage'][\"cpu\"]\n",
    "                    mem_usage=pod['containers'][0]['usage'][\"memory\"]\n",
    "                    if cpu_usage!='0':\n",
    "                        cpu_usage = int(cpu_usage[:-1])# remove the 'n' suffix\n",
    "                    if mem_usage!='0':\n",
    "                        mem_usage = int(mem_usage[:-2])# remove the 'Ki' suffix\n",
    "                    #convert Ki to Mi and n to m\n",
    "                    mem_usage*=0.000976562\n",
    "                    cpu_usage*=0.000001    \n",
    "                    print(cpu_usage,mem_usage)\n",
    "        \n",
    "        curr_bottlenecked_resource_utilization=min(cpu_usage,mem_usage)#need this value in 'm' if cpu is considered or in Mi if memory is a bottleneck\n",
    "        curr_rps= 525000             #can fetch the current rps from prometheus\n",
    "        max_workload_per_pod=(max_workload*curr_rps)/curr_bottlenecked_resource_utilization # in rps \n",
    "        return max_workload_per_pod\n",
    "\n",
    "\n",
    "\n",
    "    # Helper function to get the current number of replicas\n",
    "    def get_replica_count():\n",
    "        \n",
    "        try:\n",
    "            deployment = app_api.read_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
    "            #pprint(deployment)\n",
    "        except ApiException as e:\n",
    "            print(\"Exception when calling AppsV1Api->read_namespaced_deployment: %s\\n\" % e)\n",
    "        \n",
    "        #deployment = app_api.read_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE)\n",
    "        return deployment.spec.replicas\n",
    "\n",
    "    # Helper function to scale up\n",
    "    def scale_up(replicas):\n",
    "        app_api.patch_namespaced_deployment_scale(\n",
    "            name=DEPLOYMENT_NAME,\n",
    "            namespace=NAMESPACE,\n",
    "            body={\"spec\": {\"replicas\": int(replicas)}}\n",
    "        )\n",
    "        print(f\"Scaled up to {int(replicas)} replicas\")\n",
    "\n",
    "    # Helper function to scale down\n",
    "    def scale_down(replicas):\n",
    "        app_api.patch_namespaced_deployment_scale(\n",
    "            name=DEPLOYMENT_NAME,\n",
    "            namespace=NAMESPACE,\n",
    "            body={\"spec\": {\"replicas\": int(replicas)}}\n",
    "        )\n",
    "        print(f\"Scaled down to {int(replicas)} replicas\")\n",
    "\n",
    "    # Main loop\n",
    "    if True:\n",
    "        max_workload_per_pod = calculate_max_workload()\n",
    "        print(\"max workload per pod: \", max_workload_per_pod)\n",
    "        predicted_future_pods=predicted_workload/max_workload_per_pod\n",
    "        print(\"predicted future pods: \",predicted_future_pods)\n",
    "        current_pods=get_replica_count()\n",
    " \n",
    "        \n",
    "\n",
    "        if predicted_future_pods>current_pods:\n",
    "            results+=','+str(int(predicted_future_pods))\n",
    "            scale_up(predicted_future_pods)\n",
    "        elif predicted_future_pods<current_pods:\n",
    "            #print(\"future pods\",future_pods)\n",
    "            future_pods=max(predicted_future_pods,pods_minimum)\n",
    "            pods_surplus=(current_pods-future_pods)*RRS\n",
    "            future_pods_for_scaling_down=current_pods-pods_surplus\n",
    "            results+=','+str(int(future_pods_for_scaling_down))\n",
    "            scale_down(future_pods_for_scaling_down)\n",
    "        else:\n",
    "            print(\"No scaling required\")\n",
    "        #time.sleep(INTERVAL)\n",
    "#     savetxt('results.csv', data, delimiter=',')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea567481",
   "metadata": {},
   "source": [
    "# Load testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6d2488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful requests: 100000/100000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "def make_request(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        # You can customize this based on your application's response\n",
    "        if response.status_code == 200:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_concurrent_requests(url, num_requests):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_requests) as executor:\n",
    "        # Use a list to store the futures\n",
    "        futures = [executor.submit(make_request, url) for _ in range(num_requests)]\n",
    "\n",
    "        # Wait for all futures to complete\n",
    "        concurrent.futures.wait(futures)\n",
    "\n",
    "        # Get the results\n",
    "        results = [future.result() for future in futures]\n",
    "\n",
    "    # Count successful requests\n",
    "    successful_requests = sum(results)\n",
    "    print(f\"Successful requests: {successful_requests}/{num_requests}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your endpoint and the number of concurrent requests\n",
    "    endpoint_url = \"http://localhost:9999/hello\"\n",
    "    num_concurrent_requests = 100000\n",
    "\n",
    "    test_concurrent_requests(endpoint_url, num_concurrent_requests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bcb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
